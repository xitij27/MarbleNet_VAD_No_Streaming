import sys
import os
from os.path import basename, isdir, isfile, join
import logging
import torch
import hydra
import time
import yaml

from torchmetrics import ConfusionMatrix
import nemo.collections.asr as nemo_asr
import pytorch_lightning as pl
from nemo.utils.exp_manager import exp_manager
from omegaconf import OmegaConf
from omegaconf import open_dict

from src.vad.data_prep.audio_processing.wrapper_for_soundfile import SoundfileWrapper
from src.vad.data_prep.audio_processing.read_chunked_audio_files import ReadTrim
from src.vad.data_prep.annotations import Annotations

def chunking(sampled_data_path,save_to_folder):

    """Process raw data files and perform audio chunking for later inference.

    This function reads in raw data files from the 'sampled_config_60mins/' directory and processes them.
    It initializes an Annotations object and loads annotations from the files in the sampled data path.
    Any keys in the annotations dictionary with empty values are removed from the dictionary.
    The Annotations object is used to create a SoundfileWrapper object, which performs segmentation
    and saves the segmented audio chunks to the 'chunked_audio/' directory.

    Returns:
        tuple: A tuple containing:
            - dict: A dictionary containing the loaded annotations with non-empty values.
            - str: The path to the folder where the chunked audio files are saved.

    Raises:
        Exception: If there are any errors during the process, they will be logged but not raised directly.

    Note:
        Make sure the 'sampled_config_60mins/' and 'chunked_audio/' directories exist and contain the required files.

    Example:
        # Usage of the chunking function
        inference_files = chunking(sample_path, save_path)"""

    try:
        logging.info(f"The sampled_data_path is {sampled_data_path}")
    except Exception as e:
        logging.error(f"Unable to extract sampled_data_path due to {e}")
    try:
        logging.info(f"The save_to_folder is {save_to_folder}")
    except Exception as e:
        logging.error(f"Unable to extract save_to_folder due to {e}")

    logging.info("Initialising Annotations Dictionary :")
    try:
        annote = Annotations(sampled_data_path)
        annote_dict = annote.annotations_loader()
        keys_which_are_empty = [key for key in annote_dict.keys() if not annote_dict[key]]
        for _ in keys_which_are_empty:
            del annote_dict[_]
        print(annote_dict.keys())
    except Exception as e:
        logging.error(e)

    try:
        sf_wrapper = SoundfileWrapper(
            annotations=annote_dict,
            output_dir=save_to_folder,
            durations=5, # seconds
        )
        sf_wrapper.segmentation_loader()
    except Exception as e:
        logging.error(
            f'segmentation has failed with error: {e}')
    logging.info("Data preparation pipeline has ended, please check the logs for any anomaly.")
    inference_files = read_chunked_audio_files(save_to_folder,annote)
    return inference_files

def read_chunked_audio_files(data_folder_head,annote):
    """Reads and prepares the chunked audio files for inference.

    This function reads the chunked audio files generated by the 'ReadTrim' class in the specified 'data_folder_head'
    directory. It prepares the list of JSON files containing annotation data to be used for inference.

    Args:
        data_folder_head (str): The path to the folder where the chunked audio files are saved.
        annote (Annotations): An instance of the Annotations class containing annotation data.

    Returns:
        str: A comma-separated string containing the paths to the JSON files containing annotation data.

    Example:
        # Usage of the read_chunked_audio_files function
        data_folder = "chunked_audio/"
        annotation_obj = Annotations("sampled_config_60mins/")
        inference_files = read_chunked_audio_files(data_folder, annotation_obj)
    """
    read_trim = ReadTrim(data_folder_head, annote)
    read_trim.handle_generated_folders(
        duration=5, manifest_folder_path=data_folder_head # Chunk size seconds
    )
    json_files = [os.path.join(data_folder_head,file) for file in os.listdir(data_folder_head) if ".json" in file]
    inference_files = ','.join(json_files)
    return inference_files

def model_eval(inference_files = None,save_to_folder = None):
    """Evaluate the MarbleNet Lite model on the provided audio files.

    This function evaluates the MarbleNet Lite model on the chunked audio files specified in the 'inference_files'.
    The model is loaded from the './MarbleNet-3x2x64.nemo' checkpoint file and then tested on the provided audio data.
    The evaluation results are logged, and the predicted labels and ground truth labels are returned.

    Args:
        inference_files (str): A comma-separated string containing the paths to JSON files with annotation data
                                for the chunked audio files used for inference.
        save_to_folder (str): The path to the folder where the chunked audio files are saved.

    Returns:
        torch.Tensor: A tensor containing the predicted labels.
        torch.Tensor: A tensor containing the ground truth labels.

    Example:
        # Usage of the model_eval function
        inference_files = "file1.json,file2.json,file3.json"
        save_to_folder = "chunked_audio/"
        predicted_labels, ground_truth_labels = model_eval(inference_files, save_to_folder)
    """
    config_path = (
        "./marblenet_lite.yaml"
    )
    config = OmegaConf.load(config_path)
    config = OmegaConf.to_container(config, resolve=True)
    config = OmegaConf.create(config)
    config.model.test_ds.manifest_filepath = inference_files
    model = nemo_asr.models.EncDecClassificationModel.restore_from(restore_path="./MarbleNet-3x2x64.nemo")
    # model.cfg.labels = config.model.labels
    model.setup_test_data(config.model.test_ds)
    test_dl = model._test_dl
    vad_model = model.cpu()
    vad_model.eval()
    with torch.no_grad():
        logits, labels = extract_logits(vad_model, test_dl)
        _, pred = logits.topk(1, dim=1, largest=True, sorted=True)
        pred = pred.squeeze()
        metric = ConfusionMatrix(num_classes=2, task='binary')
        logging.info(metric(pred, labels))
        return pred, labels

class ReverseMapLabel:
    """Helper class to map prediction and label indices back to their original labels.

    This class is designed to be used as a callable object, which takes prediction and label indices as input
    and returns their corresponding original labels. It performs a reverse mapping from index to label using
    the provided 'data_loader' containing the label-to-index and index-to-label dictionaries.

    Args:
        data_loader (torch.utils.data.DataLoader): The DataLoader containing the dataset used for training or testing.

    Attributes:
        label2id (dict): A dictionary mapping original labels to their corresponding integer indices.
        id2label (dict): A dictionary mapping integer indices to their corresponding original labels.

    Example:
        # Usage of the ReverseMapLabel class
        from torch.utils.data import DataLoader

        # Assume 'my_data_loader' is a DataLoader object containing your dataset
        reverse_map = ReverseMapLabel(my_data_loader)

        # Suppose 'pred_idx' and 'label_idx' are the predicted and label indices respectively
        pred_label, true_label = reverse_map(pred_idx, label_idx)
    """
    def __init__(self, data_loader):
        self.label2id = dict(data_loader.dataset.label2id)
        self.id2label = dict(data_loader.dataset.id2label)

    def __call__(self, pred_idx, label_idx):
        return self.id2label[pred_idx], self.id2label[label_idx]

def extract_logits(model, dataloader):
    """Extract logits from the model for each batch in the dataloader.

    This function processes the data in 'dataloader' in batches using the provided 'model'
    and extracts the logits (model's raw outputs) for each batch. It also collects the corresponding
    ground truth labels from the dataloader.

    Args:
        model (torch.nn.Module): The model to use for inference.
        dataloader (torch.utils.data.DataLoader): The DataLoader containing the data for inference.

    Returns:
        torch.Tensor: A tensor containing the concatenated logits for all batches.
        torch.Tensor: A tensor containing the concatenated ground truth labels for all batches.

    Example:
        test_dl = model._test_dl
        model = model.from_pretrained(model.ckpt)
        logits, labels = extract_logits(model, test_dl)
    """
    logits_buffer = []
    label_buffer = []
    count = 0
    for batch in dataloader:
        count += 1
        print(count)
        audio_signal, audio_signal_len, labels, labels_len = batch
        logits = model(input_signal=audio_signal, input_signal_length=audio_signal_len)
        logits_buffer.append(logits)
        label_buffer.append(labels)
    print()

    logging.info("Finished extracting logits !")
    logits = torch.cat(logits_buffer, 0)
    labels = torch.cat(label_buffer, 0)
    return logits, labels


if __name__ == "__main__":
    logging.info("starting inference")
    start_time = time.time()
    # feel free to change the sampled_config_60mins/ to anything other folder of sample you would like to have
    sampled_data_path = "sampled_config_60mins/"
    save_to_folder = "chunked_audio/"
    inference_files = chunking(sampled_data_path,save_to_folder)
    pred,labels = model_eval(inference_files,save_to_folder)
    time_now = time.time()
    time_used = time_now - start_time
    logging.info(f"time used = {time_used} seconds")
    pred_list = pred.tolist()
    label_list = labels.tolist()
    labels_pred = [(lab,pre) for lab,pre in zip(label_list,pred_list)]
    with open(os.path.join(save_to_folder,"result.txt"),'a') as r:
        r.write("label,Pred \n")
        for line in labels_pred:
            r.write(f"{line[0]},{line[1]}\n")
    
    from sklearn.metrics import roc_auc_score

    # Read the file and parse the data
    with open(os.path.join(save_to_folder,"result.txt"),'r') as file:
        lines = file.readlines()

    actual_labels = []
    predicted_labels = []
    for line in lines[1:]:  # Skip the header line
        parts = line.strip().split(',')
        actual_labels.append(int(parts[0]))
        predicted_labels.append(int(parts[1]))

    # Calculate accuracy
    accuracy = sum(1 for true, pred in zip(actual_labels, predicted_labels) if true == pred) / len(actual_labels)

    # Calculate false alarm rate (FAR)
    false_positives = sum(1 for true, pred in zip(actual_labels, predicted_labels) if pred == 1 and true == 0)
    total_negatives = sum(1 for label in actual_labels if label == 0)
    FAR = false_positives / total_negatives

    # Calculate missed detection rate (MDR)
    false_negatives = sum(1 for true, pred in zip(actual_labels, predicted_labels) if pred == 0 and true == 1)
    total_positives = sum(1 for label in actual_labels if label == 1)
    MDR = false_negatives / total_positives

    # Calculate ROC-AUC
    roc_auc = roc_auc_score(actual_labels, predicted_labels)

    # Print results
    print("Accuracy:", accuracy)
    print("False Alarm Rate (FAR):", FAR)
    print("Missed Detection Rate (MDR):", MDR)
    print("ROC-AUC:", roc_auc)
